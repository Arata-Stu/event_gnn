data:
  name: dsec
  data_path: ./datasets/
  batch_size: 64
    
  num_workers: 
    train: 4
    val: 4
    test: 4

  augmentations:
    trans: 0.1 ## 平行移動の最大ピクセル数 width * aug_trans height * aug_trans
    zoom: 1.5 ## ズームの最大倍率
    p_flip: 0.5 ## 水平方向の反転確率

model:
  
  name: dagr
  batch_size: ${data.batch_size}
  use_image: true
  no_events: false
  img_net: resnet50
  pretrain_cnn: false
  num_scales: 2
  img_net_checkpoint : null

  base_width: 0.5
  after_pool_width: 1
  net_stem_width: 1
  yolo_stem_width: 1

  conv:
    edge_attr_dim: 2
    aggr: sum
    kernel_size: 5
    activation: relu

  pool: 
    pooling_dim_at_output: 5x7
    keep_temporal_ordering: true
    pooling_aggr: max

  ev_graph:
    radius: 0.01
    max_neighbors: 16
    max_queue_size: 128

training:
  precision: 16
  max_epochs: 100
  max_steps: 400000
  lr: 0.0002
  weight_decay: 0.00001
  limit_train_batches: 1.0
  gradient_clip_val: 1.0
  lr_scheduler:
      use: True
      total_steps: ${..max_steps}
      pct_start: 0.005
      div_factor: 25 # init_lr = max_lr / div_factor
      final_div_factor: 10000 # final_lr = max_lr / final_div_factor (this is different from Pytorch' OneCycleLR param)

validation:
  limit_val_batches: 1.0
  val_check_interval: null # Optional[int]
  check_val_every_n_epoch: 1 # Optional[int]

logging:
  ckpt_every_n_epochs: 1
  train:
    metrics:
      compute: false
      detection_metrics_every_n_steps: null # Optional[int] -> null: every train epoch, int: every N steps
    log_model_every_n_steps: 5000
    log_every_n_steps: 500
    high_dim:
      enable: True
      every_n_steps: 5000
      n_samples: 4
  validation:
    high_dim:
      enable: True
      every_n_epochs: 1
      n_samples: 8

hardware:
  num_workers:
    train: 6
    eval: 2
  gpus: 0 # Either a single integer (e.g. 3) or a list of integers (e.g. [3,5,6])
  dist_backend: "nccl"

reproduce:
  seed_everything: null # Union[int, null]
  deterministic_flag: False # Must be true for fully deterministic behaviour (slows down training)
  benchmark: False # Should be set to false for fully deterministic behaviour. Could potentially speed up training.

  
wandb:
  #   How to use:
  #   1) resume existing wandb run:                                 set artifact_name & wandb_runpath
  #   2) resume full training state in new wandb run:               set artifact_name
  #   3) resume only model weights of checkpoint in new wandb run:  set artifact_name & resume_only_weights=True
  #
  #   In addition: you can specify artifact_local_file to load the checkpoint from disk.
  #   This is for example required for resuming training with DDP.
  wandb_runpath: null # WandB run path. E.g. USERNAME/PROJECTNAME/1grv5kg6
  artifact_name: null # Name of checkpoint/artifact. Required for resuming. E.g. USERNAME/PROJECTNAME/checkpoint-1grv5kg6-last:v15
  artifact_local_file: null # If specified, will use the provided local filepath instead of downloading it. Required if resuming with DDP.
  resume_only_weights: False
  group_name: ??? # Specify group name of the run
  project_name: EventGNN